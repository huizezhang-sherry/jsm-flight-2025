---
title: "Scraping Marathon Data"
date: "2025-03-05"
execute:
  warning: false
format: html
editor: visual
---

# Scraping Marathon Data

For now, focusing on the last ten years of data (2015-2024).

## Initialize/Load Packages

```{r}
#| label: init

rm(list = ls())
library(httr)
library(httr2)
library(jsonlite)
library(stringr)
library(dplyr)
library(tidyr)
```

## Boston Marathon (2024)

While the official race results are available on [baa.org](https://www.baa.org/races/boston-marathon/results/search-results), location data are not provided.
Therefore, a third party website ([marathonguide.com](https://www.marathonguide.com/results/browse.cfm?MIDD=25512240415&year=2024)) which does have location data was used.
Within the US, city/state is provided for each runner; outside the US, only country.
Marathonguide.com retrieves this data from runzy.com, but each retrieval is only 20 rows at a time.
I used ChatGPT to iteratively query data - with a 2 second timeout between retrievals, since the server was timing out otherwise.

STATUS AS OF 3/20/25: ALL BOSTON DATA SCRAPED, BUT LOCATION DATA MISSING FOR 2021.
TRY TO FIND ANOTHER WEBSITE?

```{r}
#| label: boston_2024

# Base URL
base_url <- "https://back.runzy.com/mg/event-results/25512240415/2024"

### after 1200 (60 queries) it times out

# Parameters
params <- list(
  sort_field = "over_all_place",
  sort_direction = "asc",
  offset = 0
)

# Initialize empty list to collect data
all_results <- list()

# # Pagination loop — get 20 results at a time until no more results
# repeat {
#   # Make the request
#   response <- request(base_url) %>%
#     req_url_query(!!!params) %>%
#     req_perform() %>%
#     resp_body_json()
# 
#   # Extract results
#   results <- response$results
# 
#   # If no results, we're done
#   if (length(results) == 0) break
# 
#   # Append results to list
#   all_results <- append(all_results, results)
# 
#   # Move to next page
#   params$offset <- params$offset + 20
#   
#   # Respect the server - add a short pause (adjust if needed)
#   Sys.sleep(2)
# }
# 
# # Combine all pages into a single data frame
# results_df <- bind_rows(all_results)
# 
# # Write .csv to raw data folder
# dir <- getwd() # current directory is /scripts
# parent_dir <- dirname(dir) # get project directory
# data_dir <- paste0(parent_dir,'/data-raw/') 
#
# write.csv(results_df, paste0(data_dir,"boston2024.csv"), row.names = F)
```

```{r}
#| label: boston_rest

base_url <- "https://back.runzy.com/mg/event-results/25512240415/"
years <- 2015:2023 %>% as.character()
urls <- paste0(base_url, years)

# for (i in 1:length(urls)){
#   url <- urls[i]
#   year <- years[i]
#   
#   if (i > 7){ # just cause loop taking a while
#     # Parameters
#     params <- list(
#     sort_field = "over_all_place",
#     sort_direction = "asc",
#     offset = 0
#     )
#     
#     # Initialize empty list to collect data
#     all_results <- list()
#     
#     # Pagination loop — get 20 results at a time until no more results
#     repeat {
#     # Make the request
#     response <- request(url) %>%
#       req_url_query(!!!params) %>%
#       req_perform() %>%
#       resp_body_json()
#     
#     # Extract results
#     results <- response$results
#     
#     # If no results, we're done
#     if (length(results) == 0) break
#     
#     # Append results to list
#     all_results <- append(all_results, results)
#     
#     # Move to next page
#     params$offset <- params$offset + 20
#     
#       # Respect the server - add a short pause (adjust if needed)
#       Sys.sleep(1)
#     }
#     
#     # Combine all pages into a single data frame
#     results_df <- bind_rows(all_results)
#     
#     # Write .csv to raw data folder
#     dir <- getwd() # current directory is /scripts
#     parent_dir <- dirname(dir) # get project directory
#     data_dir <- paste0(parent_dir,'/data-raw/') 
#     
#     write.csv(results_df, paste0(data_dir,"boston",year,".csv"), 
#               row.names = F)
#   }
# }
```

## Chicago Marathon

Data source is [here](https://chicago-history.r.mikatiming.com/2023/?page=1&event=MAR_9TGG963812D&lang=EN_CAP&pid=list&search%5Bsex%5D=M&search%5Bage_class%5D=%25)

STATUS AS OF 3/20/25: NO DATA SCRAPED YET, UNSURE WHY CODE BELOW DIDN'T WORK

```{r}
# 
# # Base URL for the list page (male & female, all age classes)
# base_url <- "https://chicago-history.r.mikatiming.com/2023/?page=%d&event=MAR_9TGG963812D&lang=EN_CAP&pid=list&search[sex]=&search[age_class]=%%25"
# 
# # Base URL for the detail page
# detail_base_url <- "https://chicago-history.r.mikatiming.com/2023/?content=detail&fpid=list&pid=list&idp=%s&lang=EN&event=MAR_9TGG963812D&lang=EN_CAP&search[sex]=&search[age_class]=%%25&search_event=MAR_9TGG963812D"
# 
# # Initialize storage for results
# all_runners <- list()
# 
# page_number <- 1
# 
# repeat {
#   # Construct the paginated list URL
#   list_url <- sprintf(base_url, page_number)
#   page <- read_html(list_url)
# 
#   # Extract all runner rows
#   runners <- page %>% html_elements(".list__row")
# 
#   if (length(runners) == 0) {
#     message("No more runners found - stopping.")
#     break
#   }
# 
#   # Loop through each runner on the page
#   for (runner in runners) {
#     name <- runner %>% html_element(".list__name") %>% html_text(trim = TRUE)
#     bib <- runner %>% html_element(".list__bib") %>% html_text(trim = TRUE)
#     age <- runner %>% html_element(".list__age") %>% html_text(trim = TRUE)
#     gender <- runner %>% html_element(".list__sex") %>% html_text(trim = TRUE)
#     rank <- runner %>% html_element(".list__place") %>% html_text(trim = TRUE)
#     time <- runner %>% html_element(".list__time") %>% html_text(trim = TRUE)
# 
#     # Extract profile link to get the runner ID (idp parameter)
#     detail_link <- runner %>% html_element("a") %>% html_attr("href")
#     profile_id <- str_match(detail_link, "idp=([^&]+)")[,2]
# 
#     # Visit the detail page to get City & State
#     detail_url <- sprintf(detail_base_url, profile_id)
#     detail_page <- read_html(detail_url)
# 
#     city_state <- detail_page %>% html_element(".athlete__location") %>% html_text(trim = TRUE)
# 
#     # Split into city & state if available
#     city <- NA
#     state <- NA
# 
#     if (!is.na(city_state) && str_detect(city_state, ",")) {
#       city <- str_trim(str_split(city_state, ",")[[1]][1])
#       state <- str_trim(str_split(city_state, ",")[[1]][2])
#     } else {
#       city <- city_state # Sometimes international entries may not have a state
#     }
# 
#     # Store data as a row
#     runner_data <- tibble(
#       name = name,
#       bib = bib,
#       age = age,
#       gender = gender,
#       rank = rank,
#       time = time,
#       city = city,
#       state = state,
#       profile_id = profile_id,
#       detail_url = detail_url
#     )
# 
#     all_runners <- append(all_runners, list(runner_data))
#   }
# 
#   message(sprintf("Finished page %d with %d runners.", page_number, length(runners)))
# 
#   page_number <- page_number + 1
# }
# 
# # Combine all into a single data frame
# final_results <- bind_rows(all_runners)
# 
# # Save to CSV
# write.csv(final_results, "chicago_marathon_2023_results.csv", row.names = FALSE)
# 
# # Done
# message("Scrape complete! Data saved to 'chicago_marathon_2023_results.csv'")
```

## Honolulu Marathon (2024)

We are considering adding the Honolulu marathon to this list because out of state travelers.

STATUS AS OF 3/20/25: YEARS \>2020 SCRAPED, ALL HAVE LOCATION DATA. LOOKING AT EARLIER YEARS.

2019: the [official results page](https://www.honolulumarathon.org/2019-results) links to a broken site

```{r}
#| label: honolulu_2024

# library(httr)
# library(jsonlite)
# library(dplyr)
# 
# # Initialize an empty list to store results
# all_results <- list()
# 
# # Set base URL and API key
# base_url <- "https://5b8btxj9jd.execute-api.us-west-2.amazonaws.com/public/results"
# api_key <- "Wk5cgF3bQh12Rfg81N9f2a077mqe1tP24br0MZBG"
# 
# # Loop over pages
# page <- 0
# page_size <- 100
# while (TRUE) {
#   # Construct the full URL with pagination
#   url <- paste0(base_url,
#                 "?rid=142773&eid=42394&mid=732&raceid=142773&page=", page,
#                 "&pageSize=", page_size)
# 
#   # Send request with headers
#   response <- GET(url,
#                   add_headers(
#                     "x-api-key" = api_key,
#                     "User-Agent" = "Mozilla/5.0",
#                     "Referer" = "https://sportstats.one",
#                     "Origin" = "https://sportstats.one"
#                   ))
# 
#   # Check response status
#   if (status_code(response) == 200) {
#     data <- content(response, "text") |> fromJSON()
# 
#     # Stop if no more results are returned
#     if (length(data$results) == 0) break
# 
#     # Append results to the list
#     all_results <- append(all_results, data$results)
# 
#     cat("Page", page, "retrieved with", length(data$results), "results\n")
# 
#     # Increment page number for next request
#     page <- page + 1
# 
#     # Sleep to avoid rate limiting (adjust as needed)
#     Sys.sleep(2)  # 2-second delay between requests
#   } else {
#     cat("Failed on page", page, "with status:", status_code(response), "\n")
#     break
#   }
# }
# 
# # Save to csv (messy bc took a while to figure out)
# stor_mat <- matrix(NA, nrow = length(all_results), ncol = 100)
# 
# for(i in 1:length(all_results)){
#   working_row <- all_results[[i]]
#   len_working_row <- length(working_row)
#   if (length(colnames(working_row)) > 0){ # there is weird data here!
#     working_row <- rep(NA, 100)
#   }
#   else if(len_working_row < 100){ # there is not weird data, its just short!
#     working_row <- c(working_row, rep(NA,(100-len_working_row)))
#   }
#   
#   stor_mat[i,] <- working_row
# }
# 
# data_dir <- paste0(parent_dir,'/data-raw/')
# write.csv(stor_mat, paste0(data_dir,"honolulu2024.csv"), row.names = F)
```

```{r}
#| label: honolulu_2019_2023

# Year specific things (main page https://sportstats.one/event/honolulu-marathon)
year <- 2019 # done: 2021, 2022, 2023, 2024(above)
#id <- c("41729","130133") # eid, rid; 2023
#id <- c("41986","140524") # eid, rid; 2022
#id <- c("42394","114569") # eid, rid; 2021
#id <- NULL # no race in 2020
# id <- c("29414","109547") # eid, rid; 2019
# 
# rid <- id[2] 
# eid <- id[1] 
# 
# # Initialize an empty list to store results
# all_results <- list()
# 
# # Set base URL and API key
# base_url <- "https://5b8btxj9jd.execute-api.us-west-2.amazonaws.com/public/results"
# api_key <- "Wk5cgF3bQh12Rfg81N9f2a077mqe1tP24br0MZBG"
# 
# # Loop over pages
# page <- 0
# page_size <- 100
# while (TRUE) {
#   # Construct the full URL with pagination
#   url <- paste0(base_url,
#                 "?rid=",rid,
#                 "&eid=",eid,
#                 "&mid=732&raceid=",rid,
#                 "&page=", page,
#                 "&pageSize=", page_size)
# 
#   # Send request with headers
#   response <- GET(url,
#                   add_headers(
#                     "x-api-key" = api_key,
#                     "User-Agent" = "Mozilla/5.0",
#                     "Referer" = "https://sportstats.one",
#                     "Origin" = "https://sportstats.one"
#                   ))
# 
#   # Check response status
#   if (status_code(response) == 200) {
#     data <- content(response, "text", encoding = 'UTF-8') |> fromJSON()
# 
#     # Stop if no more results are returned
#     if (length(data$results) == 0) break
# 
#     # Append results to the list
#     all_results <- append(all_results, data$results)
# 
#     cat("Page", page, "retrieved with", length(data$results), "results\n")
# 
#     # Increment page number for next request
#     page <- page + 1
# 
#     # Sleep to avoid rate limiting (adjust as needed)
#     Sys.sleep(2)  # 2-second delay between requests
#   } else {
#     cat("Failed on page", page, "with status:", status_code(response), "\n")
#     break
#   }
# }
# 
# # Save to CSV
# 
# stor_mat <- matrix(NA, nrow = length(all_results), ncol = 100)
# 
# for(i in 1:length(all_results)){
#   working_row <- all_results[[i]]
#   len_working_row <- length(working_row)
#   if (length(colnames(working_row)) > 0){ # there is weird data here!
#     working_row <- rep(NA, 100)
#   }
#   else if(len_working_row < 100){ # there is not weird data, its just short!
#     working_row <- c(working_row, rep(NA,(100-len_working_row)))
#   }
# 
#   stor_mat[i,] <- working_row
# }
# 
# parent_dir <- dirname(getwd())
# data_dir <- paste0(parent_dir,'/data-raw/')
# write.csv(stor_mat, paste0(data_dir,"honolulu",year,".csv"), row.names = F)
```

## New York Marathon

For the New York Marathon, runner location data are available on [the official race website](https://results.nyrr.org/event/M2024/finishers).
However, scraping may be difficult (tbd)

TODO: 2021

```{r}

url <- "https://results.nyrr.org/event/M2024/finishers"

```

## 
